---
title: "EP 960 Latent Class Analysis"
author: "David Kaplan"
output:
  html_document: default
pdf_document: default

---
  
```{r,echo=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

# Install "poLCA"
install.packages("poLCA")

```{r,echo=TRUE}
library(poLCA)
```

## Read in data
```{r,echo=TRUE}
reading <- read.csv("~/desktop/reading.csv",header=T)
```
## Select time 3 (fall 1st grade) variables

```{r,echo=TRUE}
readvars <- subset(reading,select=c(letterrec3,beginning3,ending3,words3,reading3))
```

The data are acutally longitudinal, but we are selecting one time point.

## Recode so that values are 1 and 2 instead of 0 and 1.  2 is mastery
```{r,echo=TRUE}
readvars <- readvars + 1
```

## Specify the formula for the model with no predictors
```{r,echo=TRUE}
readtime1 <- cbind(letterrec3,beginning3,ending3,words3,reading3) ~ 1 
```

This provides the code for a simple latent class analysis without regressing latent class membership onto a predictor.


# Testing three latent class models.
```{r,echo=TRUE}
m1 <-  poLCA(readtime1,readvars,nclass=1,nrep=20,maxiter=500)
```
The loglinear model of independence amongst the cells is rejected.  There is dependency amongst the cell counts that we wish to explain through latent classes.

```{r,echo=TRUE}
m2 <-  poLCA(readtime1,readvars,nclass=2,nrep=20,maxiter=500,graphs=T)
```

The two-class model is preferred over the one-class model on the basis of the BIC.  The results show that Class 1 is made up of children who have all of the precursor skills to full reading, but are not quite mastering reading words separately and in context. These children make up approximately 27% of the sample. Class 2 is made of up children in the fall of 1st grade who have mastered letter recognition and beginning sounds as well as a majority who have mastered ending sounds. These children make up approximately 73% of the sample.  


```{r,echo=TRUE}
m3 <-  poLCA(readtime1,readvars,nclass=3,nrep=20,maxiter=500,graphs=T)
```

For the three-class model, Class 1 (63%) shows a moderate degree of mastery of necessary reading skills before full reading. Class 2 (26%) shows mastery of skills up to in early reading.  Class 3 (11%) show only letter recognition mastery.  The substantive interpretation of these results depend on knowledge of the survey sampling and population.  The three-class model is preferred on the basis of the BIC. 

## Compare entropy values.

###  Entropy is a number taking a minumum value of 0 (representing complete concentration of probability on one cell) and a maximum value equal to the logarithm of the total number of cells in the fitted cross-classfication table (representing complete dispersion, or equal probability for outcomes across every cell). Thus, lower values indicate better separation. NOTE:  Mplus rescales entropy to [0,1], where values of .8 or greater indicate better separation.

```{r,echo=TRUE}
poLCA.entropy(m1)
poLCA.entropy(m2)
poLCA.entropy(m3)
```

We find that Model 3 shows the clearest class separation as measured by the entropy coefficient, but it is not much lower than the two-class model. 

## Add SES to the file of reading variables

```{r,echo=TRUE}
attach(reading)
readT1ses <- cbind(readvars,ses)
```
## Latent class regression with SES

```{r,echo=TRUE}
read1reg <- cbind(letterrec3,beginning3,ending3,words3,reading3) ~ ses
m4 <- poLCA(read1reg,readT1ses,nclass=3,nrep=20,maxiter=500,graphs=T)
poLCA.entropy(m4)
```

First, note that the class labels have switched.  Also, the class proportions have changed a bit, indicating the importance of SES as a predictor of class membership.  The bottom of the output is a multinomial regression.  The predictor is SES, and the outcome are the log odds of membership in group 2 v. group 1 and also group 3 v. group 1.  If we name the Class 1 as the low reading mastery class, then for higher levels of ses there is a higher probability of being in Class 2 than in Class 1 and a higher probability of being in Class 3 than in Class 1.  These can be converted to odds ratios if you wish.
